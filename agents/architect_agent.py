from langchain_core.messages import SystemMessage, HumanMessage
from core.state import AgentState
from config_factory import get_llm

def architect_node(state: AgentState):
    print("\n [Architekt]: Planuj struktur plik贸w...")
    
    # Jeli to kontynuacja rozmowy, nie musimy generowa struktury od nowa,
    # chyba 偶e u偶ytkownik o to prosi. (Dla uproszczenia tutaj generujemy zawsze).
    
    requirements = state.get("requirements", "")
    plan = state.get("plan", [])
    plan_str = plan[-1] if plan else ""
    
    llm = get_llm(model_role="coder")
    
    system_prompt = """Jeste Architektem Systemu.
    Na podstawie wymaga wypisz list plik贸w niezbdnych do dziaania projektu.
    
    Zasady:
    1. Wypisz TYLKO nazwy plik贸w (np. main.py, utils.py).
    2. Ka偶da nazwa w nowej linii.
    3. Nie dodawaj opis贸w ani numeracji.
    """
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Wymagania: {requirements}\nPlan: {plan_str}")
    ]
    
    response = llm.invoke(messages)
    
    # Parsowanie: dzielimy po liniach i usuwamy puste
    files = [line.strip() for line in response.content.split('\n') if line.strip() and not line.startswith("#")]
    
    print(f" [Architekt]: Zaplanowano {len(files)} plik贸w: {files}")
    
    return {
        "file_structure": files,
        "current_file_index": 0, # Resetujemy licznik plik贸w na start
        "project_files": [],     # Resetujemy pliki (nowy start)
        "messages": [response]
    }