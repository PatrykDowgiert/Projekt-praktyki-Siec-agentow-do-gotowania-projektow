from langchain_core.messages import SystemMessage, HumanMessage
from core.state import AgentState
from config_factory import get_llm

def pm_node(state: AgentState):
    """
    Rola: Product Manager
    Zadanie: Analiza wymagaÅ„ i stworzenie listy zadaÅ„ (User Stories).
    """
    print("\nğŸ•µï¸  [PM]: AnalizujÄ™ wymagania...")
    
    requirements = state.get("requirements", "")
    
    # Pobieramy model skonfigurowany jako 'pm' (np. llama3.3 lub qwen)
    llm = get_llm(model_role="pm")
    
    system_prompt = """JesteÅ› doÅ›wiadczonym Product Managerem w zespole Agile.
    TwÃ³j cel: Przeanalizuj wymagania uÅ¼ytkownika i stwÃ³rz zwiÄ™zÅ‚Ä… listÄ™ zadaÅ„ (User Stories) niezbÄ™dnych do realizacji projektu.
    
    Zasady:
    1. KaÅ¼de zadanie powinno byÄ‡ konkretne.
    2. Nie pisz kodu, tylko opisz funkcjonalnoÅ›Ä‡.
    3. Wynik zwrÃ³Ä‡ jako punktowanÄ… listÄ™.
    """
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Oto wymagania projektu:\n{requirements}")
    ]
    
    response = llm.invoke(messages)
    plan_content = response.content
    
    print(f"ğŸ•µï¸  [PM]: StworzyÅ‚em plan dziaÅ‚ania (Backlog).")
    
    # Aktualizujemy stan: zapisujemy plan i dodajemy wiadomoÅ›Ä‡ do historii
    return {
        "plan": [plan_content],
        "messages": [response]
    }