import os
import httpx
from dotenv import load_dotenv
from langchain_ollama import ChatOllama, OllamaEmbeddings

# adowanie zmiennych z .env
load_dotenv()

# Pobieranie konfiguracji
BASE_URL = os.getenv("OLLAMA_BASE_URL")
TOKEN = os.getenv("OLLAMA_TOKEN")
VERIFY_SSL = os.getenv("OLLAMA_VERIFY_SSL", "True").lower() == "true"

# Przygotowanie nag贸wk贸w (Authorization)
HEADERS = {
    "Authorization": f"Bearer {TOKEN}"
}

# Przygotowanie klienta HTTP (obsuga SSL)
# To jest kluczowe dla Twojego rodowiska korporacyjnego
http_client = httpx.Client(verify=VERIFY_SSL)

def get_llm(model_role="coder", temperature=0.2):
    """
    Zwraca skonfigurowan instancj ChatOllama w zale偶noci od roli.
    """
    # Wyb贸r modelu na podstawie roli
    if model_role == "coder":
        model_name = os.getenv("MODEL_CODER", "qwen3-coder:30b")
    elif model_role == "pm":
        # U偶ywamy llama3.3 lub innego 'mdrego' modelu do zarzdzania
        model_name = os.getenv("MODEL_PM", "llama3.3:70b") 
    else:
        model_name = os.getenv("MODEL_CODER")

    print(f" Inicjalizacja LLM: {model_name} dla roli: {model_role}")

    llm = ChatOllama(
        base_url=BASE_URL,
        model=model_name,
        temperature=temperature,
        # Kluczowe dla Twojego setupu: przekazanie klienta i nag贸wk贸w
        client_args={
            "headers": HEADERS,
            "verify": VERIFY_SSL # Przekazujemy verify=False jeli tak jest w .env
        }
    )
    return llm

def get_embeddings():
    """
    Zwraca skonfigurowan instancj do Embedding贸w (dla RAG).
    """
    model_name = os.getenv("MODEL_EMBEDDING", "embeddinggemma:300m")
    
    embeddings = OllamaEmbeddings(
        base_url=BASE_URL,
        model=model_name,
        client_args={
            "headers": HEADERS,
            "verify": VERIFY_SSL
        }
    )
    return embeddings